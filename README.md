# RFP Analysis System

An advanced system for analyzing Request for Proposal (RFP) and tender documents using AI and NLP technologies.

## ğŸ—ï¸ Project Structure

### Current Components

```
RFP-Analysis-System/
â”œâ”€â”€ ğŸ“„ app.py                    # Streamlit web interface
â”œâ”€â”€ ğŸ“„ document_qa.py            # Document processing and QA logic
â”œâ”€â”€ ğŸ“„ vector_store.py           # Vector storage implementations (FAISS)
â”œâ”€â”€ ğŸ“„ pattern_store.py          # Pattern matching and extraction
â”œâ”€â”€ ğŸ“„ patterns.json             # Predefined patterns for extraction
â”œâ”€â”€ ğŸ“„ requirements.txt          # Project dependencies
â”œâ”€â”€ ğŸ“„ .env                      # Environment configuration
â”œâ”€â”€ ğŸ“„ qa_responses.md           # Response templates
â”œâ”€â”€ ğŸ“ __pycache__/             # Python cache directory
â””â”€â”€ ğŸ“„ Data Files
    â”œâ”€â”€ DCA_formatted.txt       # Document Control Agreement
    â”œâ”€â”€ RFP_formatted.txt       # Request for Proposal
    â”œâ”€â”€ sample.txt             # Sample test document
    â””â”€â”€ sample_document.txt    # Sample test document

```

## ğŸ”‘ Key Components

### 1. Document Processing (`document_qa.py`)
- **DocumentChunker Class**
  - Implements hybrid chunking strategy
  - Hierarchical section-based chunking
  - Semantic enhancement layer
  - Context enrichment with metadata
  - Special handling for definitions and tables
  - Chunk size optimization (1000 tokens max)

### 2. Vector Storage (`vector_store.py`)
- **VectorStore Class**
  - FAISS integration for similarity search
  - OpenAI embeddings integration
  - Advanced QA chain setup
  - Response formatting
  - N/A filtering
  
- **LocalVectorStore Class**
  - Local vector storage implementation
  - Cosine similarity search
  - Metadata management
  
- **MongoVectorStore Class**
  - MongoDB-based vector storage
  - Distributed storage capability
  - Index management

### 3. Pattern Management (`pattern_store.py`)
- **Pattern extraction and matching**
  - Technical requirement patterns
  - Financial requirement patterns
  - Legal clause patterns
  - Cross-reference handling

### 4. Web Interface (`app.py`)
- **Streamlit Application**
  - Document upload interface
  - Interactive Q&A system
  - Result visualization
  - Document processing status
  - Sample question suggestions

### 5. Configuration
- **Environment Setup (.env)**
  - API keys management
  - Model configurations
  - System settings

### 6. Data Management
- **Pattern Definitions (patterns.json)**
  - Predefined extraction patterns
  - Regular expressions
  - Entity definitions
  
- **Response Templates (qa_responses.md)**
  - Standard response formats
  - Answer templates
  - Citation formats

## ğŸ› ï¸ Technical Implementation

### Vector Store Integration
- FAISS for efficient similarity search
- Pinecone for distributed vector storage
- MongoDB for metadata and document storage

### LLM Integration
- OpenAI GPT models for text analysis
- Custom prompt engineering
- Context window optimization
- Response formatting

### Document Structure Handling
- Hierarchical parsing
- Cross-reference management
- Table and list preservation
- Metadata enrichment

## ğŸ“Š Data Flow

1. **Document Ingestion**
   ```
   Upload â†’ OCR/Text Extraction â†’ Preprocessing â†’ Chunking
   ```

2. **Analysis Pipeline**
   ```
   Chunks â†’ Embedding â†’ Vector Storage â†’ Query Processing â†’ Response Generation
   ```

3. **Quality Assurance**
   ```
   Response â†’ Validation â†’ Cross-reference Check â†’ Format Check â†’ Final Output
   ```

## ğŸ”§ Configuration

### Chunking Settings
- Maximum chunk size: 1000 tokens
- Overlap: 100 tokens
- Minimum: Complete semantic unit
- Hierarchical boundaries: Parts, Articles, Sections

### Vector Store Settings
- Dimension: 768 (default)
- Similarity metric: Cosine
- Index type: FAISS IVF
- Batch size: 1000

### LLM Settings
- Model: GPT-3.5-turbo-16k
- Temperature: 0.0 (precise extraction)
- Max tokens: 2000
- Presence/frequency penalty: 0.0

## ğŸš€ Getting Started

1. **Installation**
   ```bash
   pip install -r requirements.txt
   ```

2. **Environment Setup**
   ```bash
   cp .env.example .env
   # Configure your API keys and settings
   ```

3. **Running the Application**
   ```bash
   streamlit run app.py
   ```

## ğŸ“ Usage Guidelines

1. **Document Preparation**
   - Ensure clean, searchable PDF/text format
   - Verify page numbers and section headers
   - Check table formatting

2. **Query Formation**
   - Be specific with requirements
   - Include relevant section references
   - Use document terminology

3. **Result Interpretation**
   - Verify page number citations
   - Cross-check with source document
   - Review all related clauses

## ğŸ” Quality Assurance

- Cross-reference verification
- Answer completeness checking
- Technical accuracy validation
- Format consistency
- Source citation validation

## ğŸ›¡ï¸ Security Considerations

- API key protection
- Rate limiting
- Input validation
- Audit logging
- Access control

## ğŸ“ˆ Performance Optimization

- Caching strategies
- Batch processing
- Asynchronous operations
- Query optimization
- Memory management

## ğŸ¤ Contributing

Please read [CONTRIBUTING.md](CONTRIBUTING.md) for details on our code of conduct and the process for submitting pull requests.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details.

## ğŸ”„ System Flow

### 1. Document Processing Flow
```mermaid
graph TD
    A[Upload Document] --> B[Document Loading]
    B --> C[Text Extraction]
    C --> D[Preprocessing]
    D --> E[Hybrid Chunking]
    
    subgraph Chunking Process
        E --> F[Section-Based Split]
        F --> G[Semantic Enhancement]
        G --> H[Context Enrichment]
    end
    
    H --> I[Vector Embedding]
    I --> J[FAISS Storage]
```

### 2. Query Processing Flow
```mermaid
graph LR
    A[User Query] --> B[Query Analysis]
    B --> C[Pattern Matching]
    C --> D[Vector Search]
    
    subgraph Retrieval Process
        D --> E[Top K Chunks]
        E --> F[Context Assembly]
        F --> G[LLM Processing]
    end
    
    G --> H[Response Formatting]
    H --> I[Display Results]
```

### 3. Detailed Component Interaction

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    app.py       â”‚     â”‚  document_qa.py â”‚     â”‚ vector_store.py â”‚
â”‚  Web Interface  â”‚â”€â”€â”€â”€â–¶â”‚    Processor    â”‚â”€â”€â”€â”€â–¶â”‚  Vector Store   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  patterns.json  â”‚     â”‚ pattern_store.pyâ”‚     â”‚    OpenAI API   â”‚
â”‚    Patterns     â”‚â—€â”€â”€â”€â–¶â”‚Pattern Matching â”‚     â”‚  Embeddings/LLM â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4. Data Flow Description

1. **Document Upload & Processing**
   ```
   Document â†’ Text Extraction â†’ Chunking â†’ Embedding â†’ Storage
   ```
   - Upload via Streamlit interface (app.py)
   - Processing in document_qa.py
   - Vector storage in vector_store.py

2. **Query Processing**
   ```
   Query â†’ Pattern Match â†’ Vector Search â†’ Context Retrieval â†’ LLM â†’ Response
   ```
   - Query input through Streamlit
   - Pattern matching using pattern_store.py
   - Vector search in FAISS store
   - LLM processing with OpenAI

3. **Response Generation**
   ```
   Context â†’ Template â†’ Format â†’ Citations â†’ Display
   ```
   - Context assembly from chunks
   - Template application
   - Citation addition
   - Formatting and display

### 5. Key Process Steps

1. **Document Processing**
   - Load document using document_qa.DocumentChunker
   - Apply hybrid chunking strategy
   - Generate embeddings using OpenAI
   - Store in FAISS index

2. **Query Handling**
   - Process user query
   - Match against patterns.json
   - Search vector store
   - Retrieve relevant chunks

3. **Response Assembly**
   - Combine chunk information
   - Apply response templates
   - Add citations and references
   - Format for display

4. **Quality Control**
   - Filter N/A responses
   - Validate citations
   - Check completeness
   - Ensure formatting

### 6. System Integration Points

1. **External APIs**
   ```
   OpenAI API â†â†’ vector_store.py
   ```
   - Embeddings generation
   - LLM query processing

2. **Storage Integration**
   ```
   FAISS â†â†’ vector_store.py
   ```
   - Vector storage
   - Similarity search

3. **Pattern Management**
   ```
   patterns.json â†â†’ pattern_store.py
   ```
   - Pattern definition
   - Pattern matching

4. **User Interface**
   ```
   app.py â†â†’ All Components
   ```
   - User interaction
   - Result display
